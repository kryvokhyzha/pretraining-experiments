# Production training configuration for full-scale pretraining using Hydra instantiate
_target_: transformers.TrainingArguments
output_dir: outputs
seed: 0
num_train_epochs: 1
per_device_train_batch_size: 8
per_device_eval_batch_size: 8
learning_rate: 5e-5
weight_decay: 0.01
warmup_ratio: 0.1
lr_scheduler_type: cosine
gradient_checkpointing: true
dataloader_num_workers: 8
bf16: true
gradient_accumulation_steps: 4 # Effective batch size = 32

# Optimizer configuration
optim: adamw_torch

# Logging and evaluation configuration
logging_steps: 25
save_steps: 1000
eval_strategy: steps
eval_steps: 500
save_total_limit: 3

# Additional training settings
remove_unused_columns: false
ddp_find_unused_parameters: false
dataloader_pin_memory: true
group_by_length: true
