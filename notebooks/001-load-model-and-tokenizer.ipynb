{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc018b7",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fcce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import joblib\n",
    "import rootutils\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3e7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_root = rootutils.find_root(indicator=\".project-root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e84f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model: AutoModelForCausalLM) -> str:\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "\n",
    "    return (\n",
    "        f\"\\ntrainable model parameters: {trainable_model_params}\"\n",
    "        f\"\\nall model parameters: {all_model_params}\"\n",
    "        f\"\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a0973",
   "metadata": {},
   "source": [
    "## Experiments with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319c3ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-3-270m\")\n",
    "tokenizer_modified = AutoTokenizer.from_pretrained(\"transhumanist-already-exists/tereshchenkoblue-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09cf95e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens from Gemma tokenizer: [('▁pede', 139534), ('▁disproportion', 58670), ('▁mindset', 39927), ('xt', 903), ('isque', 105598)]\n",
      "Vocab sizes: Gemma = 262145, Modiified = 262145\n"
     ]
    }
   ],
   "source": [
    "# token -> id\n",
    "vocab_gemma = tokenizer_gemma.get_vocab()\n",
    "vocab_modified = tokenizer_modified.get_vocab()\n",
    "\n",
    "print(\"Sample tokens from Gemma tokenizer:\", list(vocab_gemma.items())[:5])\n",
    "print(f\"Vocab sizes: Gemma = {len(vocab_gemma)}, Modiified = {len(vocab_modified)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46fe05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse maps id -> token\n",
    "inv_gemma = {v: k for k, v in vocab_gemma.items()}\n",
    "inv_modified = {v: k for k, v in vocab_modified.items()}\n",
    "\n",
    "tokens_gemma = set(vocab_gemma.keys())\n",
    "tokens_modified = set(vocab_modified.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfc1e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<image_soft_token>', '<pad>', '<image_soft_token>')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_gemma[0], inv_gemma[262144], inv_modified[0], inv_modified[262144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4654af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap analysis\n",
    "shared_tokens = tokens_gemma & tokens_modified\n",
    "only_in_gemma = tokens_gemma - tokens_modified\n",
    "only_in_modified = tokens_modified - tokens_gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c4b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY:\n",
      "  vocab_size_1: 262145\n",
      "  vocab_size_2: 262145\n",
      "  shared_token_count: 187598\n",
      "  same_token_same_id: 180656\n",
      "  same_token_diff_id: 6942\n",
      "  same_id_count: 262145\n",
      "  same_id_same_token: 180656\n",
      "  same_id_diff_token: 81489\n",
      "  only_in_1: 74547\n",
      "  only_in_2: 74547\n"
     ]
    }
   ],
   "source": [
    "same_token_same_id = 0\n",
    "same_token_diff_id = 0\n",
    "for t in shared_tokens:\n",
    "    id_gemma = vocab_gemma[t]\n",
    "    id_modified = vocab_modified[t]\n",
    "    if id_gemma == id_modified:\n",
    "        same_token_same_id += 1\n",
    "    else:\n",
    "        same_token_diff_id += 1\n",
    "\n",
    "# same numeric id that maps to different tokens\n",
    "shared_ids = set(inv_gemma.keys()) & set(inv_modified.keys())\n",
    "same_id_diff_token = sum(1 for i in shared_ids if inv_gemma[i] != inv_modified[i])\n",
    "same_id_same_token = sum(1 for i in shared_ids if inv_gemma[i] == inv_modified[i])\n",
    "\n",
    "# Summary\n",
    "summary = {\n",
    "    \"vocab_size_1\": len(vocab_gemma),\n",
    "    \"vocab_size_2\": len(vocab_modified),\n",
    "    \"shared_token_count\": len(shared_tokens),\n",
    "    \"same_token_same_id\": same_token_same_id,\n",
    "    \"same_token_diff_id\": same_token_diff_id,\n",
    "    \"same_id_count\": len(shared_ids),\n",
    "    \"same_id_same_token\": same_id_same_token,\n",
    "    \"same_id_diff_token\": same_id_diff_token,\n",
    "    \"only_in_1\": len(only_in_gemma),\n",
    "    \"only_in_2\": len(only_in_modified),\n",
    "}\n",
    "\n",
    "print(\"SUMMARY:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6d6718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same id and same token: [0, 1, 2, 256002, 256003, 262144] (total 180656)\n"
     ]
    }
   ],
   "source": [
    "# tokens_same_id_and_same_token = [inv_gemma[i] for i in shared_ids if inv_gemma[i] == inv_modified[i]]\n",
    "ids_same_id_and_same_token = [i for i in shared_ids if inv_gemma[i] == inv_modified[i]]\n",
    "print(\n",
    "    f\"Same id and same token: {ids_same_id_and_same_token[:3] + ids_same_id_and_same_token[-3:]}\"\n",
    "    f\" (total {len(ids_same_id_and_same_token)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e3b1a",
   "metadata": {},
   "source": [
    "## Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b509aeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trainable model parameters: 268098176\n",
      "all model parameters: 268098176\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model_gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-270m\")\n",
    "print(print_number_of_trainable_model_parameters(model_gemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f910ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77371bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([262144, 640])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gemma.get_input_embeddings().weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f641cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid IDs: 180655\n",
      "Number of invalid IDs: 1\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(sorted(ids_same_id_and_same_token), dtype=torch.long)\n",
    "\n",
    "valid_mask = t < model_gemma.get_input_embeddings().weight.shape[0]\n",
    "invalid_ids = t[~valid_mask]\n",
    "ids_to_freeze = t[valid_mask]\n",
    "\n",
    "joblib.dump(ids_to_freeze, path_to_root / \"data\" / \"freeze_ids_gemma_tereshchenko.joblib\")\n",
    "\n",
    "print(f\"Number of valid IDs: {len(ids_to_freeze)}\")\n",
    "print(f\"Number of invalid IDs: {len(invalid_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abbb36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hook(grad, ids):\n",
    "    if grad is None:\n",
    "        return None\n",
    "\n",
    "    # Make a copy of the gradient tensor to avoid in-place modification issues\n",
    "    # But probably this is not necessary\n",
    "    grad = grad.clone()\n",
    "    grad[ids] = 0\n",
    "    return grad\n",
    "\n",
    "\n",
    "hook_handle = model_gemma.get_input_embeddings().weight.register_hook(partial(_hook, ids=ids_to_freeze))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "615a7d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gemma.get_input_embeddings().weight[0][0].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "500b7e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0),\n",
       " torch.Size([262144, 640]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gemma.get_input_embeddings(), model_gemma.get_input_embeddings().weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607996b",
   "metadata": {},
   "source": [
    "## Quick smoke test\n",
    "\n",
    "+ run a forward+backward and confirm grads were zeroed for those ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b29a28e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total grad L1: 2780.26708984375, freezed grad L1: 0.0\n"
     ]
    }
   ],
   "source": [
    "model_gemma.train()\n",
    "\n",
    "input_text = \"Hello world, мене звати Роман, я хочу дещо протестувати.\"\n",
    "enc = tokenizer_gemma(input_text, return_tensors=\"pt\")\n",
    "outputs = model_gemma(**enc, labels=enc[\"input_ids\"])\n",
    "\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "emb = model_gemma.get_input_embeddings()\n",
    "\n",
    "if emb.weight.grad is None or ids_to_freeze.numel() == 0:\n",
    "    total_grad_l1 = 0.0\n",
    "else:\n",
    "    row_grad_l1 = emb.weight.grad.abs().sum(dim=1).detach().cpu()\n",
    "    ids_cpu = ids_to_freeze.cpu().to(torch.long)\n",
    "    freezed_grad_l1 = float(row_grad_l1[ids_cpu].sum().item())\n",
    "    total_grad_l1 = float(row_grad_l1.sum().item())\n",
    "\n",
    "print(f\"Total grad L1: {total_grad_l1}, freezed grad L1: {freezed_grad_l1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e822d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrainable model parameters: 268098176\\nall model parameters: 268098176\\npercentage of trainable model parameters: 100.00%'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_number_of_trainable_model_parameters(model_gemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a63f9e",
   "metadata": {},
   "source": [
    "## We can't just set `requires_grad=False` for specific rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20c1aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param is leaf: True\n",
      "param.requires_grad: True\n",
      "view is leaf: False\n",
      "view.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "emb = model_gemma.get_input_embeddings()\n",
    "print(\"param is leaf:\", emb.weight.is_leaf)\n",
    "print(\"param.requires_grad:\", emb.weight.requires_grad)\n",
    "\n",
    "# indexing returns a view (non-leaf) that inherits requires_grad\n",
    "x = emb.weight[0]\n",
    "print(\"view is leaf:\", x.is_leaf)\n",
    "print(\"view.requires_grad:\", x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db5911ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt to set requires_grad_ on view failed as expected:\n",
      "- RuntimeError(\"you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().\")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    view = emb.weight[ids_to_freeze[0]]\n",
    "    view.requires_grad_(False)\n",
    "except Exception as e:\n",
    "    print(\"Attempt to set requires_grad_ on view failed as expected:\\n-\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a804ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretraining-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
